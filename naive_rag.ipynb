{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudo de Naive RAG\n",
    "Naive RAG study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Estas instruções foram retiradas do link: <https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2>_\n",
    "\n",
    "\n",
    "The above link was used to follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação de Pacotes\n",
    "Package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observação:** Pode ser necessário instalar novos pacotes ao longo das instruções\n",
    "- **Note:** It may be necessary to install new packages throughout the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain openai weaviate-client tiktoken python-dotenv pdfplumber\n",
    "# pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports gerais\n",
    "General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar variáveis de ambiente\n",
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Chave API do OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando pdfplumber para extrair o texto que servirá como contexto para a IA\n",
    "Using pdfplumber to extract the text that will serve as context for the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open('./your_file.pdf') as pdf:\n",
    "    text = ''\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o conteúdo em um arquivo / Save content into a file\n",
    "with open('pdf_file_text.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar contexto\n",
    "Load context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este é o contexto que será fornecido à **LLM**\n",
    "- This is the context that will be provided to the **LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Usar contexto a partir de uma URL // Context from URL\n",
    "url = \"https://gist.githubusercontent.com/fzliu/973bb1d659a740b1d78a659f90be4a02/raw/6ff89ece130119011951f450164006221acadd8a/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "\n",
    "loader = TextLoader(\"./state_of_the_union.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentar o documento de texto (criar _chunks_)\n",
    "Create chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- É necessário segmentar o texto em pedaços menores porque seu conteúdo é muito longo para caber na janela de contexto da **LLM**\n",
    "- Será usada a lib **LangChain** para isso\n",
    "\n",
    "- It is necessary to segment the text into smaller chunks because its content is too long to fit in the **LLM** context window\n",
    "- The **LangChain** library will be used for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporar (_embed_) e armazenar os segmentos\n",
    "Embed and store chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para haver busca semântica pelos segmentos, é preciso gerar as incorporações em forma de vetor (_vector embeddings_) para cada segmento e armazená-las junto com suas incorporações\n",
    "- Será usada a **OpenAI** para gerar os vetores incorporados e o **Weaviate vector db** para armazenar os vetores\n",
    "\n",
    "- For semantic search of the segments, it is necessary to generate vector embeddings for each segment and store them along with their embeddings\n",
    "- OpenAI will be used to generate the vector embeddings and Weaviate vector db to store the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,\n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Passo 1:** _Retrieve_ (Recuperar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uma vez que a base de dados vetorizada (_vector database_) está populada, é possível defini-la como o **componente de recuperação**, que busca um contexto adicional baseado na similaridade semântica entre a _query_ do usuário e os segmentos incorporados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Passo 2:** _Augment_ (Aprimorar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para aprimorar o prompt com o contexto adicional, é preciso preparar um **template do prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Você é um assistente para tarefas de perguntas e respostas.\\nUtilize os seguintes fragmentos de contexto recuperado para responder à pergunta.\\nSe você não souber a resposta, simplesmente diga que não sabe.\\nNão poupe palavras ou tokens na resposta.\\nPergunta: {question}\\nContexto: {context}\\nResposta:\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\n",
    "Utilize os seguintes fragmentos de contexto recuperado para responder à pergunta.\n",
    "Se você não souber a resposta, simplesmente diga que não sabe.\n",
    "Não poupe palavras ou tokens na resposta.\n",
    "Pergunta: {question}\n",
    "Contexto: {context}\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Passo 3:** _Generate_ (Gerar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agora é possível gerar uma cadeia para o pipeline RAG, juntando o **retriever**, o **prompt** e a **LLM**\n",
    "- Quando a cadeia RAG estiver definida, é possível invocá-la\n",
    "\n",
    "- Now it is possible to generate a chain for the RAG pipeline, combining the **retriever**, the **prompt**, and the **LLM**\n",
    "- Once the RAG chain is defined, it can be invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romul/Docs/github_ia/rag_basico/env/lib/python3.11/site-packages/pydantic/main.py:1114: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/Users/romul/Docs/github_ia/rag_basico/env/lib/python3.11/site-packages/pydantic/main.py:1114: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, max_tokens=16000)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"\"\"Me explique, em português, qual é o ponto principal do discurso.\"\"\"\n",
    "\n",
    "output = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Me explique, em português, qual é o ponto principal do discurso.\n",
      "Resposta: O ponto principal do discurso é a afirmação da força e resiliência da nação americana diante das adversidades. O orador expressa otimismo em relação ao futuro dos Estados Unidos, destacando que, apesar dos desafios enfrentados, o caráter e a determinação do povo americano são fundamentais para superar crises e transformar dificuldades em oportunidades. O discurso enfatiza a importância da liberdade, da justiça e da responsabilidade coletiva, além de reafirmar a posição dos Estados Unidos como uma nação que sempre busca expandir a democracia e a equidade. O orador também faz referência ao apoio à Ucrânia em sua luta pela liberdade, simbolizando a luta global contra a opressão. Em suma, o discurso é um chamado à ação e à unidade, ressaltando que a força da nação reside em seu povo.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pergunta: {query}\")\n",
    "print(f\"Resposta: {output}\")\n",
    "\n",
    "with open('output.md', 'w', encoding='utf-8') as file:\n",
    "    file.write(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
